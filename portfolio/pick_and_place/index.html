<!DOCTYPE html>
<html lang='en'><head>
  <title>Robotic Pick and Place Task Based on Visual Affordance Model - Zhizhuo Zhang</title>
  <link rel='canonical' href='https://ZhizhuoUp.github.io/portfolio/pick_and_place/' />
  <meta charset='utf-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1' />
  <meta name='description' content='This is the description of our sample project' />
  <meta name='theme-color' content='#FD3519' />
  

  <meta name="generator" content="Hugo 0.124.1">

  





<link rel="stylesheet" href="https://ZhizhuoUp.github.io/sass/style.min.eabe1aa4bd266a15f7b39b122bd6a5cc75cb067e5373631ac21d7815d6240d6f.css" integrity="sha256-6r4apL0mahX3s5sSK9alzHXLBn5Tc2Mawh14FdYkDW8=" media="screen">
<link rel="stylesheet" href="https://ZhizhuoUp.github.io/syntax.min.css" integrity="" media="screen">

  <meta property="og:title" content="Robotic Pick and Place Task Based on Visual Affordance Model" />
<meta property="og:description" content="This is the description of our sample project" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ZhizhuoUp.github.io/portfolio/pick_and_place/" /><meta property="article:section" content="portfolio" />



  <meta itemprop="name" content="Robotic Pick and Place Task Based on Visual Affordance Model">
<meta itemprop="description" content="This is the description of our sample project">

<meta itemprop="wordCount" content="509">
<meta itemprop="keywords" content="" />
</head>
<body>

  <header style="background-image:linear-gradient(
      rgba(0,0,0,0.4),rgba(0,0,0,0.4)
    ),url(&#39;https://ZhizhuoUp.github.io/images/default-sidebar.jpg&#39;)">
  <div class="intro">
    <div class="logo-container">
      <a href="/">
        <img src='https://ZhizhuoUp.github.io/images/zzz.png' alt="Profile POST" class="rounded-logo">
      </a>
    </div>
    <h2>Zhizhuo Zhang</h2>
    <h3> </h3>
    <div class="menu">
      

        <p>
            <a href="/about/">
                About
            </a>
        </p>

        <p>
            <a href="/portfolio/">
                Portfolio
            </a>
        </p>

      
        
        <p>
            <a href="mailto:zz3012@columbia.edu" target="_blank" rel="external">
                Contact
            </a>
        </p>
      
    </div>
  </div>

  <div class="socials">
      
  
    <a href="https://github.com/ZhizhuoUp/" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM71.182 98.494c-2.156.385-2.952-.95-2.952-2.053 0-1.386.051-8.471.051-14.195 0-4.005-1.335-6.546-2.9-7.881C74.878 73.313 84.89 72.003 84.89 55.6c0-4.671-1.669-7.007-4.39-10.01.436-1.105 1.9-5.648-.436-11.552-3.568-1.104-11.731 4.595-11.731 4.595-3.389-.95-7.06-1.438-10.679-1.438-3.62 0-7.29.488-10.679 1.438 0 0-8.163-5.699-11.73-4.595-2.337 5.878-.899 10.422-.437 11.551-2.72 3.004-4.004 5.34-4.004 10.011 0 16.326 9.574 17.712 19.072 18.765-1.232 1.104-2.336 3.003-2.72 5.724-2.44 1.104-8.677 3.004-12.4-3.568-2.335-4.056-6.545-4.39-6.545-4.39-4.159-.05-.282 2.619-.282 2.619 2.772 1.283 4.723 6.212 4.723 6.212 2.49 7.624 14.4 5.057 14.4 5.057 0 3.568.052 9.37.052 10.422 0 1.104-.77 2.438-2.952 2.053C27.21 92.821 15.35 76.701 15.35 57.86c0-23.564 18.02-41.456 41.585-41.456s42.663 17.892 42.663 41.456c.026 18.842-11.474 34.988-28.416 40.635zM46 82.81c-.488.103-.95-.102-1.001-.436-.051-.385.282-.719.77-.822.488-.05.95.154 1.001.488.077.334-.257.668-.77.77zm-2.439-.23c0 .333-.385.615-.898.615-.565.052-.95-.23-.95-.616 0-.333.385-.616.899-.616.487-.051.95.231.95.616zm-3.516-.283c-.103.334-.616.488-1.053.334-.488-.103-.821-.488-.719-.822.103-.334.617-.488 1.053-.385.513.154.847.54.719.873zm-3.158-1.386c-.23.282-.718.23-1.104-.154-.385-.334-.487-.822-.23-1.053.23-.282.718-.23 1.103.154.334.334.462.847.231 1.053zm-2.336-2.336c-.23.154-.667 0-.95-.385-.282-.385-.282-.822 0-1.001.283-.231.72-.052.95.333.283.385.283.847 0 1.053zm-1.668-2.49c-.231.23-.616.103-.899-.154-.282-.334-.333-.719-.102-.899.23-.23.616-.102.898.154.282.334.334.72.103.899zm-1.72-1.9c-.103.231-.436.283-.719.103-.334-.154-.488-.436-.385-.667.103-.154.385-.231.719-.103.334.18.488.462.385.667z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://www.linkedin.com/in/zhizhuo-zhang-934813247/" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M106.786 0H8.189C3.67 0 0 3.722 0 8.291v98.418C0 111.278 3.67 115 8.189 115h98.597c4.518 0 8.214-3.722 8.214-8.291V8.29C115 3.722 111.304 0 106.786 0zm-72.03 98.571H17.713V43.69h17.07V98.57h-.025zm-8.522-62.377c-5.467 0-9.882-4.44-9.882-9.883 0-5.442 4.415-9.882 9.882-9.882 5.442 0 9.883 4.44 9.883 9.882a9.87 9.87 0 0 1-9.883 9.883zm72.414 62.377H81.604V71.875c0-6.366-.129-14.555-8.856-14.555-8.882 0-10.242 6.931-10.242 14.093V98.57H45.46V43.69h16.352v7.495h.23c2.285-4.312 7.855-8.856 16.147-8.856 17.25 0 20.458 11.372 20.458 26.158V98.57z"/>
  
  </svg>
</div>
</a>
  

  </div>
</header>

<div class="mobile-header">
  <p> Zhizhuo Zhang </p>
  <div class="hamburger">
    <div class="bar"></div>
    <div class="bar"></div>
    <div class="bar"></div>
  </div>
</div>

<div class="overlay-menu">
  <header>
    <div class="intro">
      <div class="logo-container">
        <a href="/">
          <img src='https://ZhizhuoUp.github.io/images/zzz.png' alt="Profile POST" class="rounded-logo">
        </a>
      </div>
      <h2>Zhizhuo Zhang</h2>
      <h3> </h3>
      <div class="menu">
        

        <p>
            <a href="/about/">
                About
            </a>
        </p>

        <p>
            <a href="/portfolio/">
                Portfolio
            </a>
        </p>

        
          
          <p>
              <a href="mailto:zz3012@columbia.edu" target="_blank" rel="external">
                  Contact
              </a>
          </p>
        
      </div>
    </div>

    <div class="socials">
        
  
    <a href="https://github.com/ZhizhuoUp/" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM71.182 98.494c-2.156.385-2.952-.95-2.952-2.053 0-1.386.051-8.471.051-14.195 0-4.005-1.335-6.546-2.9-7.881C74.878 73.313 84.89 72.003 84.89 55.6c0-4.671-1.669-7.007-4.39-10.01.436-1.105 1.9-5.648-.436-11.552-3.568-1.104-11.731 4.595-11.731 4.595-3.389-.95-7.06-1.438-10.679-1.438-3.62 0-7.29.488-10.679 1.438 0 0-8.163-5.699-11.73-4.595-2.337 5.878-.899 10.422-.437 11.551-2.72 3.004-4.004 5.34-4.004 10.011 0 16.326 9.574 17.712 19.072 18.765-1.232 1.104-2.336 3.003-2.72 5.724-2.44 1.104-8.677 3.004-12.4-3.568-2.335-4.056-6.545-4.39-6.545-4.39-4.159-.05-.282 2.619-.282 2.619 2.772 1.283 4.723 6.212 4.723 6.212 2.49 7.624 14.4 5.057 14.4 5.057 0 3.568.052 9.37.052 10.422 0 1.104-.77 2.438-2.952 2.053C27.21 92.821 15.35 76.701 15.35 57.86c0-23.564 18.02-41.456 41.585-41.456s42.663 17.892 42.663 41.456c.026 18.842-11.474 34.988-28.416 40.635zM46 82.81c-.488.103-.95-.102-1.001-.436-.051-.385.282-.719.77-.822.488-.05.95.154 1.001.488.077.334-.257.668-.77.77zm-2.439-.23c0 .333-.385.615-.898.615-.565.052-.95-.23-.95-.616 0-.333.385-.616.899-.616.487-.051.95.231.95.616zm-3.516-.283c-.103.334-.616.488-1.053.334-.488-.103-.821-.488-.719-.822.103-.334.617-.488 1.053-.385.513.154.847.54.719.873zm-3.158-1.386c-.23.282-.718.23-1.104-.154-.385-.334-.487-.822-.23-1.053.23-.282.718-.23 1.103.154.334.334.462.847.231 1.053zm-2.336-2.336c-.23.154-.667 0-.95-.385-.282-.385-.282-.822 0-1.001.283-.231.72-.052.95.333.283.385.283.847 0 1.053zm-1.668-2.49c-.231.23-.616.103-.899-.154-.282-.334-.333-.719-.102-.899.23-.23.616-.102.898.154.282.334.334.72.103.899zm-1.72-1.9c-.103.231-.436.283-.719.103-.334-.154-.488-.436-.385-.667.103-.154.385-.231.719-.103.334.18.488.462.385.667z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://www.linkedin.com/in/zhizhuo-zhang-934813247/" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M106.786 0H8.189C3.67 0 0 3.722 0 8.291v98.418C0 111.278 3.67 115 8.189 115h98.597c4.518 0 8.214-3.722 8.214-8.291V8.29C115 3.722 111.304 0 106.786 0zm-72.03 98.571H17.713V43.69h17.07V98.57h-.025zm-8.522-62.377c-5.467 0-9.882-4.44-9.882-9.883 0-5.442 4.415-9.882 9.882-9.882 5.442 0 9.883 4.44 9.883 9.882a9.87 9.87 0 0 1-9.883 9.883zm72.414 62.377H81.604V71.875c0-6.366-.129-14.555-8.856-14.555-8.882 0-10.242 6.931-10.242 14.093V98.57H45.46V43.69h16.352v7.495h.23c2.285-4.312 7.855-8.856 16.147-8.856 17.25 0 20.458 11.372 20.458 26.158V98.57z"/>
  
  </svg>
</div>
</a>
  

    </div>
  </header>
</div>

  <div class="content-wrapper">
    
      <div class="breadcrumb">
  





<span >
  <a href="https://ZhizhuoUp.github.io/">HOME</a>
   / 
</span>


<span >
  <a href="https://ZhizhuoUp.github.io/portfolio/">PORTFOLIO</a>
   / 
</span>


<span  class="active">
  <a href="https://ZhizhuoUp.github.io/portfolio/pick_and_place/">Robotic Pick and Place Task Based on Visual Affordance Model</a>
  
</span>

</div>

    
    <main id="content" class="portfolio">

<h1>Robotic Pick and Place Task Based on Visual Affordance Model</h1>
<div class="tags">
  





<div class="portfolio-tags">
  <div class="icon">
  <svg width="18px" height="18px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M77.577 51.23a1.807 1.807 0 0 0-2.2.342l-27.562 27.79a1.807 1.807 0 0 1-2.2.342l-14.008-9.702a1.807 1.807 0 0 0-2.2.342l-1.952 1.968c-.287.22-.456.568-.455.936.001.37.172.716.46.934L45.637 86.77a1.807 1.807 0 0 0 2.2-.342l31.709-31.97c.287-.22.456-.567.455-.936a1.175 1.175 0 0 0-.46-.933l-1.963-1.36z"/><path d="M97.304 20H80.512c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.347.023-.685.04-1.026H34.579c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.22.019-.434.025-.652a9.788 9.788 0 0 0-5.697 4.471 9.683 9.683 0 0 0-2.65 4.764L1.158 92.871c-.965 4.689 2.6 8.503 7.948 8.503h6.334v2.673c-.077 5.41 4.263 9.861 9.705 9.953h72.16c5.438-.095 9.774-4.546 9.694-9.953V29.953c.08-5.407-4.256-9.858-9.695-9.953zM10.078 96.653c-2.378 0-3.964-1.697-3.535-3.782L16.637 43.84h80.787L87.331 92.871a5.254 5.254 0 0 1-5.091 3.782H10.078zm91.535 7.394c.036 2.403-1.891 4.382-4.308 4.424h-72.16c-2.42-.04-4.352-2.018-4.32-4.424v-2.673h60.443c5.348 0 10.484-3.814 11.449-8.503l8.897-43.215v54.391z"/><path d="M34.814 33c1.243 0 2.251-1.057 2.251-2.36 0-1.305-1.008-2.362-2.25-2.362-2.04 0-4.313-3.194-4.313-7.778s2.272-7.778 4.312-7.778c1.227 0 2.536 1.163 3.386 3.084H43C41.716 11.19 38.578 8 34.814 8 29.871 8 26 13.49 26 20.5c0 7.009 3.871 12.5 8.814 12.5z"/>
  
  </svg>
</div>

  <span>Feb. 2023 - May. 2023 - Columbia University in the City of New York</span>
</div>
</div>

<img title="Robotic Pick and Place Task Based on Visual Affordance Model" alt="Robotic Pick and Place Task Based on Visual Affordance Model" src="/portfolio/sample-project/PAP_cover.png" />

<p>In this project, I implemented the pick and place tasks in an environment with multiple objects. The robot arm can recognize objects it wants to grasp in a messy environment and try to grasp them with appropriate postures. After that, the robot arm can avoid obstacles while moving and place objects inside the box.</p>
<p>The whole task can be divided into several steps:</p>
<h3 id="human-demonstration">Human Demonstration</h3>
<p>Considering this task only includes top-down grasping, where the pose of the gripper is reduced to three degrees of freedom (2D translation and 1D rotation), the transnational degrees of freedom are naturally encoded in the 2D pixel coordinates. To encode the rotational degree of freedom, we rotate the image observation in the opposite direction of the gripper
rotation.</p>
<p>We load images in the training dataset and label the correct rotation of every object to be grasped in each image.</p>
<div align="center">
<img src="../sample-project/PAP_label.png" style="zoom:50%" alt="aaa"/>
</div>
<h3 id="data-augmentation">Data Augmentation</h3>
<p>Randomly translate and rotate each image in the dataset and add the Gaussian heatmap to increase the diversity of the data in the training dataset and avoid model overfitting.</p>
<h3 id="implement-customized-u-net">Implement Customized U-Net</h3>
<p>I chose to customize the U-Net architecture, using RGB images and corresponding ground truth postures obtained from a simulated camera. The U-Net utilizes skip connections that allow information from the contracting path to be directly propagated to the corresponding layer in the expanding path. This helps in preserving spatial information and mitigating the vanishing gradient problem.</p>
<div align="center">
<img src="../sample-project/PAP_architecture.png" style="zoom:50%" alt="aaa"/>
</div>
<p>In this project, Visual Affordance is defined as a per-pixel value between 0 and 1. The network will return a map with the same shape as the input image, which represents whether the pixel (or the action directly mapped to this pixel) on the map is graspable.</p>
<div align="center">
<img src="../sample-project/PAP_eval.png" style="zoom:50%" alt="aaa"/>
</div>
<p>As we can see in the image shown below, the model can locate the position of the target object and select the best one in the candidate rotations.</p>
<p>Additionally, if the robot arm tries to use the best candidate rotation to grasp and fail, it can also choose another candidate based on the highest value on the map.</p>
<h3 id="trajectory-planning">Trajectory Planning</h3>
<p>To improve the ability of trajectory planning, we add the RRT algorithm to the robot arm, which means the robot can avoid obstacles while moving from one bin to another.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/oSiSbHtTGbg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h3 id="demonstration-in-pybullet">Demonstration in Pybullet</h3>
<p>We evaluate the model in a Pybullet environment, where the robot arm can gradually empty the bin filled with sundries. In 25 attempts, the robot arm removed 12 objects.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/5GL2I9WKcgk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h3 id="comparison-with-the-baseline">Comparison with the Baseline</h3>
<p>In the visual affordance formulation above, aligning the action with images allows us to effectively discretize the action space. In contrast, in this problem, we explore an action regression approach, where the output is a vector and the input is the same as that in the Visual Affordance Model.</p>
<p>To be more specific, we will use the model to predict the vector (pos_x, pos_y, rotation) for each input image with each dimension normalized to between 0 and 1.</p>
<p>When other parameters are the same as the solution using the Visual Affordance model, the robot arm only removed 1 object in 25 attempts.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/0hQpEs6uzGY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>




  <h3>Testimonial</h3>
  <div class="testimonial-list">
    
        <div class="testimonial-container">
  <img class="rounded-img" src="/portfolio/sample-project/zzz.png" alt="" title="" />
  <p class="testimonial-name"></p>
  <p class="testimonial-role"></p>
  <blockquote class="testimonial-text">The Visual Affordance Model can use the affordance map to generate a more dense learning signal compared with other alternatives. Additionally, the U-Net with the architecture based on encoder-decoder left a great impression on me because it greatly expanded my horizons in image processing and recognition, especially instance segmentation.</blockquote>
</div>

    
  </div>


    </main>
  </div>
  <footer>
    <div class="footer-wrapper">
      <p>Made with ❤️ &mdash; Powered by <a href="https://gohugo.io/" target="_blank" rel="external">Hugo</a> and the <a href="https://github.com/bjacquemet/personal-web" target='_blank' rel="external">Personal Web</a> theme. Icons come from the great <a href="https://fontawesome.com/license" target="_blank" rel="external">Font Awesome</a> library</p>
      <p>© Zhizhuo Zhang</p>
    </div>
  </footer>
  <link href="https://fonts.googleapis.com/css?family=Montserrat:500,600|Raleway:400,400i,600" rel="stylesheet">
  
  <script type="text/javascript">
    document.querySelector('.mobile-header').addEventListener('click', function () {
      var om = document.querySelector(".overlay-menu");
      if (document.querySelector('.hamburger').classList.contains("cross")) {
        document.querySelector('.hamburger').classList.remove("cross");
        om.style.display = "none";
        om.style.width = "0%";
        om.style.height = "0%";
      }
      else {
        document.querySelector('.hamburger').classList.add("cross");
        om.style.width = "100%";
        om.style.height = "100vh";
        om.style.display = "block";
      }
    });
  </script>
  <script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</body>
</html>
